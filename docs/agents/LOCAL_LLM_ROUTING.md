# Local LLM Routing Policy

## Overview

To optimize for cost and latency, the Antigravity agent uses a **"cheap control-plane brain"** approach. We route simple, low-risk tasks to locally available Small Language Models (SLMs) while reserving powerful remote models for complex reasoning and generation.

**Routing decisions are authoritative and must be executed as returned. Agents are not permitted to override tier selection or model choice outside the router.**

The routing decision is made by `scripts/model_router.py` based on:

1.  **Task Classification**: Is the task suitable for a small model?
2.  **Input Size**: Does the input fit comfortably within the local model's context window (with safety margin)?
3.  **Local Capability**: Is a capable local model (e.g., `llama3.2:1b`) actually running?

## Task Classes

### ✅ Local Tier (Safe for 1B-3B Models)

- `lint`: Analysis of code for stylistic or syntax errors.
- `classify`: Categorizing PRs, logs, or error messages.
- `route`: Determining which agent skill/tool to use for a user request.
- `summarize_small`: Summarizing short diffs or single-file content (< 2k tokens).
- `extract_structured`: Extracting specific fields (e.g., Jira ID, version number) from text.

### ❌ Remote Tier (Requires Large Models)

- `codegen_large`: Generating entire functions, classes, or modules.
- `architecture`: Designing system architecture or refactoring strategies.
- `deep_debug`: Analyzing complex, multi-file bugs.
- `ambiguous`: Handling vague user requests that require clarifying questions.
- `long_context`: Tasks involving large document sets or full repository scans.

## Configuration

The routing policy is defined in `.local/state/local_llm_capabilities.json`, which is generated by `scripts/local_llm_probe.py`.

### Default Thresholds

- **Max Input**: 12,000 characters (~3,000 tokens)
- **Max Output**: 512 tokens
- **Default Model**: `ollama:llama3.2:1b` (or similar detected model)

## Usage

### 1. Update Capabilities

Run the probe to detect hardware and update the policy:

```bash
python3 scripts/local_llm_probe.py
```

### 2. Check Route

Query the router for a specific task:

```bash
python3 scripts/model_router.py --task classify --input-len 500
```

**Output:**

```json
{
  "tier": "local",
  "model": "ollama:llama3.2:1b",
  "reason_codes": ["TASK_CLASS_LOCAL", "UNDER_CHAR_LIMIT", "RUNTIME_AVAILABLE"],
  "limits": {
    "max_input_chars": 12000,
    "max_output_tokens": 512
  },
  "policy_version": "1.0.0"
}
```

### 3. Evidence Logging

Every routing decision is automatically logged to `docs/evidence/<date>/routing/decision_<timestamp>_<pid>.json` for audit purposes.
